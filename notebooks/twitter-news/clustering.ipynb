{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# move directory to the root of this repo\n",
    "os.chdir('\\\\'.join(os.getcwd().split('\\\\')[:-2]))\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from simtag.filter import simtag_filter\n",
    "import pandas as pd\n",
    "\n",
    "def import_batch_parquet(batch_prefix):\n",
    "\tdfs = [pd.read_parquet(f'{batch_prefix}{i}.parquet') for i in range(5)]\n",
    "\tdf = pd.concat(dfs, ignore_index=True)\n",
    "\treturn df\n",
    "\n",
    "def store_batch_parquet(df, batch_prefix):\n",
    "\tchunk_size = len(df) // 5\n",
    "\tfor i in range(5):\n",
    "\t\tstart = i * chunk_size\n",
    "\t\tend = (i + 1) * chunk_size if i < 4 else len(df)\n",
    "\t\tdf.iloc[start:end].to_parquet(f\"{batch_prefix}{i}.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# list of tweets, already formatted in parquet format for easy loading\n",
    "df = pd.read_csv('notebooks/twitter-news/news_tweets.csv', lineterminator='\\n')\n",
    "df = df.dropna(subset='hashtags')\n",
    "df = df.reset_index()\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x : ast.literal_eval(x))\n",
    "sample_list = df['hashtags'].tolist()\n",
    "\n",
    "# extract hashtags\n",
    "hashtags_list = [x for x in df['hashtags'].dropna()]\n",
    "hashtags = list()\n",
    "[[hashtags.append(k) for k in x] for x in hashtags_list]\n",
    "hashtags = list(set(hashtags))\n",
    "hashtags = sorted(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate recommender\n",
    "engine = simtag_filter(\n",
    "    sample_list=sample_list,\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    quantization='int8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53300/53300 [10:15<00:00, 86.57it/s] \n",
      "  File \"c:\\Users\\ardit\\miniconda3\\envs\\SIMTAG\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
      "100%|██████████| 53300/53300 [10:03<00:00, 88.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# if not existing, compute and save M\n",
    "M, df_M = engine.compute_M(method='encoding')\n",
    "# store as batches, you can store as a unique file if you wish\n",
    "# store_batch_parquet(df_M, 'notebooks/twitter-news/M_')\n",
    "\n",
    "# if existing, load M\n",
    "# df_M = import_batch_parquet(batch_prefix='notebooks/twitter-news/M_')\n",
    "engine.load_M(df_M=df_M, covariate_transformation='dot_product', cluster_M=True, quantize_M=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing samples: 100%|██████████| 73137/73137 [00:57<00:00, 1277.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# if not existing, compute and save samples_encoded.parquet\n",
    "sample_vectors = engine.encode_samples(sample_list) # already quantized\n",
    "index_covariate = engine.compute_search_indexes(sample_vectors, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cryptogaming', 'cryptosport', 'cryptosportgaming', 'NftAnalysis'],\n",
       " ['nft', 'crypto'],\n",
       " ['nft', 'crypto'],\n",
       " ['CryptoNews', 'NFTs'],\n",
       " ['NFT', 'Crypto']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tag_list = ['crypto', 'nft']\n",
    "indices, search_results = engine.covariate_search(index_covariate, sample_list, query_tag_list=query_tag_list, allow_new_tags=True, print_new_tags=True, k=10)\n",
    "search_results[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CryptowithMC', 'Bitcoin', 'Crypto', 'CryptoNews', 'NFT']\n",
      "['nft', 'crypto', 'bitcoin']\n",
      "['NFT', 'Crypto']\n",
      "['CryptoNews', 'NFTs']\n",
      "['nft', 'crypto']\n",
      "['NFT', 'crypto']\n",
      "['nft', 'crypto']\n",
      "['nft', 'crypto']\n",
      "['cryptogaming', 'cryptosport', 'cryptosportgaming', 'NftReview']\n",
      "['cryptogaming', 'cryptosport', 'cryptosportgaming', 'NftAnalysis']\n"
     ]
    }
   ],
   "source": [
    "# M_256 does not perform well on semantic-covariate encoding\n",
    "indices, search_results = engine.semantic_covariate_search(index_covariate, sample_list, query=\"I want to buy crypto and nft\", k=10)\n",
    "for k in search_results[0:10]:\n",
    "    print(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIMTAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
