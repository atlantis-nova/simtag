{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/deeguy/twitter-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ardit\\miniconda3\\envs\\env1\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# move directory to the root of this repo\n",
    "os.chdir('\\\\'.join(os.getcwd().split('\\\\')[:-2]))\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from simtag.filter import simtag_filter\n",
    "import pandas as pd\n",
    "\n",
    "def import_batch_parquet(batch_prefix):\n",
    "\tdfs = [pd.read_parquet(f'{batch_prefix}{i}.parquet') for i in range(5)]\n",
    "\tdf = pd.concat(dfs, ignore_index=True)\n",
    "\treturn df\n",
    "\n",
    "def store_batch_parquet(df, batch_prefix):\n",
    "\tchunk_size = len(df) // 5\n",
    "\tfor i in range(5):\n",
    "\t\tstart = i * chunk_size\n",
    "\t\tend = (i + 1) * chunk_size if i < 4 else len(df)\n",
    "\t\tdf.iloc[start:end].to_parquet(f\"{batch_prefix}{i}.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# list of tweets, already formatted in parquet format for easy loading\n",
    "df = pd.read_csv('notebooks/twitter-news/news_tweets.csv', lineterminator='\\n')\n",
    "df = df.dropna(subset='hashtags')\n",
    "df = df.reset_index()\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x : ast.literal_eval(x))\n",
    "sample_list = df['hashtags'].tolist()\n",
    "\n",
    "# extract hashtags\n",
    "hashtags_list = [x for x in df['hashtags'].dropna()]\n",
    "hashtags = list()\n",
    "[[hashtags.append(k) for k in x] for x in hashtags_list]\n",
    "hashtags = list(set(hashtags))\n",
    "hashtags = sorted(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate recommender\n",
    "engine = simtag_filter(\n",
    "    sample_list=sample_list,\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not existing, compute and save M\n",
    "# M, df_M = engine.compute_M(method='encoding')\n",
    "# store as batches, you can store as a unique file if you wish\n",
    "# store_batch_parquet(df_M, 'notebooks/twitter-news/M_')\n",
    "\n",
    "# if existing, load M\n",
    "df_M = import_batch_parquet(batch_prefix='notebooks/twitter-news/M_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.load_M(df_M, covariate_transformation='dot_product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not existing, compute and save samples_encoded.parquet\n",
    "# sample_vectors = engine.encode_samples(sample_list)\n",
    "# samples_encoded = pd.DataFrame([sample_vectors], index=['vector']).T\n",
    "# store_batch_parquet(samples_encoded, 'notebooks/twitter-news/quantized_samples_encoded_')\n",
    "\n",
    "# if already existing, load samples_encoded.parquet\n",
    "samples_encoded = import_batch_parquet(batch_prefix='notebooks/twitter-news/samples_encoded_')\n",
    "sample_vectors = samples_encoded.vector.tolist()\n",
    "nbrs_covariate = engine.compute_nbrs(sample_vectors, k=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['politics', 'trump', 'corruption'],\n",
       " ['Politics', 'Political', 'Trump', 'Biden', 'FBI'],\n",
       " ['Politics', 'Trump', 'News'],\n",
       " ['News', 'Raid', 'Biden', 'Trump', 'politics', 'government'],\n",
       " ['News', 'Politics', 'Government', 'Media', 'Trump', 'Biden']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tag_dict = [ 'politics', 'trump', 'democracy' ]\n",
    "\n",
    "# perform search\n",
    "query_vector = engine.encode_query(list_tags=query_tag_dict, allow_new_tags=True, print_new_tags=True)\n",
    "indices, search_results = engine.soft_tag_filtering(nbrs_covariate, sample_list, query_vector)\n",
    "search_results[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted covariate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tag_dict = {\n",
    "    'trump' : 1,\n",
    "    'democracy' : 0.4,\n",
    "    'republicans.' : 2\n",
    "}\n",
    "\n",
    "# perform search\n",
    "query_vector = engine.encode_query(dict_tags=query_tag_dict, allow_new_tags=True, print_new_tags=True)\n",
    "indices, search_results = engine.soft_tag_filtering(nbrs_covariate, sample_list, query_vector)\n",
    "search_results[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.load_M(df_M, covariate_transformation='PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not existing, compute and save samples_encoded.parquet\n",
    "# sample_vectors = engine.encode_samples(sample_list)\n",
    "# samples_encoded = pd.DataFrame([sample_vectors], index=['vector']).T\n",
    "# store_batch_parquet(samples_encoded, 'notebooks/twitter-news/samples_encoded_PCA_')\n",
    "\n",
    "# if already existing, load samples_encoded.parquet\n",
    "samples_encoded = import_batch_parquet(batch_prefix='notebooks/twitter-news/samples_encoded_PCA_')\n",
    "sample_vectors = samples_encoded.vector.tolist()\n",
    "nbrs_covariate_pca = engine.compute_nbrs(sample_vectors, k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['republicans', 'Trump', 'Biden'],\n",
       " ['trump', 'democrats', 'news'],\n",
       " ['DemocracyNotAutocracy', 'Democrat'],\n",
       " ['Republicans', 'Democrat']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tag_dict = [ 'trump', 'Democracy', 'Republican']\n",
    "\n",
    "# perform search\n",
    "query_vector = engine.encode_query(list_tags=query_tag_dict, allow_new_tags=True, print_new_tags=True)\n",
    "indices, search_results = engine.soft_tag_filtering(nbrs_covariate_pca, sample_list, query_vector)\n",
    "search_results[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted covariate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tag_dict = {\n",
    "    'trump' : 1,\n",
    "    'democracy' : 0.4,\n",
    "    'republicans.' : 2\n",
    "}\n",
    "\n",
    "# perform search\n",
    "query_vector = engine.encode_query(dict_tags=query_tag_dict, allow_new_tags=True, print_new_tags=True)\n",
    "indices, search_results = engine.soft_tag_filtering(nbrs_covariate_pca, sample_list, query_vector)\n",
    "search_results[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "\n",
    "# df_samples = pd.DataFrame([sample_list], columns=['sample']).T\n",
    "# df_samples['vector'] = df_samples[0].progress_apply(lambda x : engine.model.encode(x)[0])\n",
    "# df_samples.to_parquet('notebooks/twitter-news/samples_regular_encoded.parquet', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# df_samples = pd.read_parquet('notebooks/twitter-news/samples_regular_encoded.parquet')\n",
    "# quantized_samples = quantize_embeddings(df_samples['vector'].values.tolist(), precision=\"int8\")\n",
    "\n",
    "# nbrs_semantic = NearestNeighbors(n_neighbors=10, metric='cosine').fit(df_samples['vector'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular semantic search\n",
    "We use a regular sentence as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WhiteHouse'],\n",
       " ['news', 'headlines'],\n",
       " ['news', 'celebrity'],\n",
       " ['News', 'Greece'],\n",
       " ['news', 'headlines'],\n",
       " ['News', 'Greece'],\n",
       " ['News', 'Greece'],\n",
       " ['News', 'Television'],\n",
       " ['news', 'stocks'],\n",
       " ['news', 'football']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = nbrs_semantic.kneighbors([engine.model.encode(\"What are the news from the white house?\")])\n",
    "indices = indices[0].tolist()\n",
    "[sample_list[x] for x in indices][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariate semantic search\n",
    "\n",
    "We use a regular sentence as an input, but on top of the covariate encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['POTUS', 'WhiteHouse', 'News'],\n",
       " ['Breitbart', 'News', 'BreitbartNews'],\n",
       " ['FoxNews', 'news'],\n",
       " ['politicsnews', 'politicalnews', 'news'],\n",
       " ['congress', 'breakingnews', 'news'],\n",
       " ['WhiteHouse'],\n",
       " ['usnews', 'news'],\n",
       " ['news', 'breakingnews', 'CNN'],\n",
       " ['news', 'breakingnews', 'CNN'],\n",
       " ['news', 'breakingnews', 'CNN']]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = nbrs_covariate.kneighbors([engine.model.encode(\"What are the news from the white house?\")])\n",
    "indices = indices[0].tolist()\n",
    "[sample_list[x] for x in indices][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semantic search with tag query\n",
    "\n",
    "We conver the query_tag_list into a string: as we can see, the queries have too much noise compared to covariate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['democrats', 'MAGA', 'maga', 'midterm', 'BLM'],\n",
       " ['Democrats', 'crime', 'police', 'Hollywood'],\n",
       " ['democrats',\n",
       "  'inflation',\n",
       "  'reelection',\n",
       "  'Republicans',\n",
       "  'recession',\n",
       "  'news',\n",
       "  'newsflash',\n",
       "  'latestupdates'],\n",
       " ['Democrats',\n",
       "  'VoteBlue',\n",
       "  'VoteBlueIn2022',\n",
       "  'VoteBlueToProtectOurRights',\n",
       "  'VoteBlueOrWeAreScrewed']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tag_list = [ 'news', 'democrats', 'republicans' ]\n",
    "\n",
    "# perform search\n",
    "query_vector = engine.model.encode(str(query_tag_list))\n",
    "indices, search_results = engine.soft_tag_filtering(nbrs_semantic, sample_list, query_vector)\n",
    "search_results[0:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
